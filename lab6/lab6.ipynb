{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2001, 2)\n",
      "Review 1st column: a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
      "Rating 2nd column: 1\n",
      "** Showing review counts\n",
      "1    1042\n",
      "0     959\n",
      "Name: 1, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************** Tokenized reviews \n",
      "0                                        [101, 1014, 102]\n",
      "1       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
      "2       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
      "3       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
      "4       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
      "                              ...                        \n",
      "1996    [101, 2205, 20857, 1998, 11865, 16643, 2135, 5...\n",
      "1997    [101, 2009, 2515, 1050, 1005, 1056, 2147, 2004...\n",
      "1998    [101, 2023, 2028, 8704, 2005, 1996, 11848, 199...\n",
      "1999    [101, 1999, 1996, 2171, 1997, 2019, 9382, 1898...\n",
      "2000    [101, 1996, 3185, 2003, 25757, 2011, 1037, 244...\n",
      "Name: 0, Length: 2001, dtype: object\n",
      "[list([101, 1014, 102])\n",
      " list([101, 1037, 18385, 1010, 6057, 1998, 2633, 18276, 2128, 16603, 1997, 5053, 1998, 1996, 6841, 1998, 5687, 5469, 3152, 102])\n",
      " list([101, 4593, 2128, 27241, 23931, 2013, 1996, 6276, 2282, 2723, 1997, 2151, 2445, 12217, 7815, 102])\n",
      " ...\n",
      " list([101, 2023, 2028, 8704, 2005, 1996, 11848, 1998, 7644, 1037, 3622, 2718, 102])\n",
      " list([101, 1999, 1996, 2171, 1997, 2019, 9382, 18988, 1998, 4089, 3006, 3085, 17312, 1010, 1996, 3750, 1005, 1055, 2252, 4332, 1037, 6397, 3239, 2000, 1996, 2200, 2381, 2009, 9811, 2015, 2000, 6570, 102])\n",
      " list([101, 1996, 3185, 2003, 25757, 2011, 1037, 24466, 16134, 2008, 1005, 1055, 2074, 6388, 2438, 2000, 7344, 3686, 1996, 7731, 4378, 2096, 13060, 18856, 17322, 2094, 2000, 15015, 10271, 4641, 102])]\n",
      "******************\n",
      "\n",
      "Getting maximum number of tokens in a sentence\n",
      "Most tokens in a review (max_len): 59\n",
      "------------\n",
      "Padded so review arrays as same size: \n",
      "These are the padded reviews:\n",
      "[[  101  1014   102 ...     0     0     0]\n",
      " [  101  1037 18385 ...     0     0     0]\n",
      " [  101  4593  2128 ...     0     0     0]\n",
      " ...\n",
      " [  101  2023  2028 ...     0     0     0]\n",
      " [  101  1999  1996 ...     0     0     0]\n",
      " [  101  1996  3185 ...     0     0     0]]\n",
      "This is the last padded sentence:\n",
      "[  101  1996  3185  2003 25757  2011  1037 24466 16134  2008  1005  1055\n",
      "  2074  6388  2438  2000  7344  3686  1996  7731  4378  2096 13060 18856\n",
      " 17322  2094  2000 15015 10271  4641   102     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0]\n",
      "\n",
      "------------\n",
      "Attention mask tells BERT to ignore the padding.\n",
      "(2001, 59)\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "=============\n",
      "Input ids which are padded reviews in torch tensor format:\n",
      "tensor([[  101,  1014,   102,  ...,     0,     0,     0],\n",
      "        [  101,  1037, 18385,  ...,     0,     0,     0],\n",
      "        [  101,  4593,  2128,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2023,  2028,  ...,     0,     0,     0],\n",
      "        [  101,  1999,  1996,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  3185,  ...,     0,     0,     0]])\n",
      "Attention mask in torch tensor format:\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "++++++++++++++\n",
      "BERT model transforms tokens and attention mask tensors into features \n",
      "for logistic regression.\n",
      "Let's see the features: \n",
      "[[-0.21887794 -0.09875865  0.24728072 ... -0.11699078  0.11915118\n",
      "   0.54785794]\n",
      " [-0.2159344  -0.14028911  0.00831101 ... -0.13694867  0.58670026\n",
      "   0.20112696]\n",
      " [-0.17262714 -0.14476176  0.00223419 ... -0.17442557  0.21386448\n",
      "   0.37197486]\n",
      " ...\n",
      " [-0.27829775 -0.24803597  0.13585812 ... -0.19039162  0.13099593\n",
      "   0.34978384]\n",
      " [-0.03667709  0.10638564 -0.01111031 ... -0.11206651  0.41619483\n",
      "   0.5033801 ]\n",
      " [ 0.12402622  0.01425178  0.01038408 ... -0.11606539  0.53459144\n",
      "   0.27495334]]\n",
      "[-3.66770886e-02  1.06385641e-01 -1.11103132e-02 -1.38567910e-01\n",
      " -7.68095106e-02 -2.27968156e-01  2.77840555e-01  2.99254388e-01\n",
      " -1.15856707e-01 -1.20137744e-01 -5.66974133e-02 -1.31957456e-01\n",
      " -9.05703232e-02  5.42803109e-01  4.71074730e-02  1.43630758e-01\n",
      " -7.58208632e-02  2.16232121e-01  2.48702005e-01  6.35917298e-03\n",
      " -3.14073175e-01 -2.52320841e-02  4.65955473e-02  2.19100997e-01\n",
      " -7.24395737e-03 -1.80428103e-03  2.30883718e-01  5.17112017e-02\n",
      " -5.20081893e-02  1.75014094e-01  2.94881016e-01 -6.67217821e-02\n",
      " -5.62939830e-02 -2.30093539e-01 -1.17651131e-02 -2.37547874e-01\n",
      "  2.04939187e-01  8.08559880e-02  1.71073854e-01  2.58265436e-01\n",
      " -9.75197405e-02  1.29212201e-01  1.76621839e-01  1.99972048e-01\n",
      "  5.37625700e-02 -3.10604423e-01 -2.84944248e+00  9.15976837e-02\n",
      " -3.37861657e-01 -3.56112063e-01  2.74362266e-01  5.61537892e-02\n",
      " -1.66375507e-02  2.81018257e-01  2.67386377e-01  3.02481443e-01\n",
      " -2.55309612e-01  4.66546983e-01  1.64326265e-01  1.19710311e-01\n",
      "  3.10860928e-02  1.64741606e-01  5.04624210e-02  4.97087687e-02\n",
      " -1.08976245e-01  3.62823397e-01  2.21914463e-02  2.92221457e-01\n",
      " -2.27436393e-01  1.17301993e-01 -2.88382560e-01 -1.98648527e-01\n",
      "  1.08603902e-01 -7.71444961e-02  2.51328778e-02 -2.11738676e-01\n",
      " -6.29469007e-02  4.28952157e-01  5.81454486e-03 -4.73321863e-02\n",
      " -1.40212998e-01  3.93889159e-01  3.78005683e-01  1.83066204e-02\n",
      " -8.96243900e-02  1.21404164e-01  2.55114865e-02  6.59907982e-02\n",
      "  2.22606778e-01  4.19071376e-01 -3.96910667e-01 -9.71677825e-02\n",
      "  1.32449150e-01  6.15185127e-02  3.02861243e-01 -5.88939965e-01\n",
      "  1.97212130e-01 -5.93817905e-02  1.79024473e-01  3.03016245e-01\n",
      "  2.65961885e-01 -2.62028545e-01  3.07184458e-02 -3.08337301e-01\n",
      "  1.69702798e-01  9.04240757e-02  1.35204688e-01 -3.99195164e-01\n",
      " -3.34847048e-02 -2.02220225e+00  2.44796395e-01  3.00940454e-01\n",
      " -6.18724935e-02 -3.38767588e-01 -1.77425683e-01  1.74245045e-01\n",
      " -1.88819878e-02 -2.14964584e-01 -1.74999107e-02 -9.81022045e-02\n",
      "  2.83264965e-01  3.36615026e-01 -6.76144287e-02 -2.89819598e-01\n",
      "  8.75422657e-02  8.73278454e-02  9.96646881e-02  2.67577488e-02\n",
      "  7.91874751e-02  4.21778187e-02  3.96565557e-01  3.35621595e-01\n",
      "  4.45982106e-02 -3.42088729e-01 -2.13344127e-01  2.59242833e-01\n",
      "  3.63603681e-01 -1.48260295e-01 -3.08127105e-01  3.92850824e-02\n",
      " -3.85714501e-01 -1.95107721e-02 -2.70435405e+00  3.13251436e-01\n",
      "  5.18772185e-01  3.40982944e-01  8.34962800e-02 -2.48438716e-02\n",
      "  9.21867043e-02 -2.33661592e-01  1.40572935e-01  2.31352895e-02\n",
      " -9.17499512e-02 -2.76518278e-02 -2.21516602e-02 -1.16210423e-01\n",
      "  1.03451326e-01 -5.42887636e-02  3.41336995e-01  1.43877834e-01\n",
      "  2.43055195e-01 -2.48773888e-01 -3.74583334e-01 -3.61056507e-01\n",
      " -2.26917967e-01  6.27722740e-02  2.72981614e-01  1.99230179e-01\n",
      " -1.25950426e-01  1.08607784e-01 -6.85848445e-02  2.39527717e-01\n",
      "  5.60019672e-01 -1.18613213e-01  3.61565143e-01  2.40534887e-01\n",
      " -6.00974225e-02 -1.22401398e-04  1.45015121e-01 -4.98201698e-02\n",
      " -2.81734705e-01  4.49657410e-01  6.45039380e-02  1.27986684e-01\n",
      " -5.85472770e-02 -2.18413338e-01  2.33083904e-01 -2.77634740e-01\n",
      " -1.69765607e-01  1.57563716e-01  3.35818678e-02 -5.99232540e-02\n",
      "  2.47080885e-02 -1.00790478e-01  3.73855203e-01  8.38487744e-02\n",
      " -9.94966775e-02 -4.41562116e-01 -7.07299560e-02 -1.22737028e-02\n",
      " -2.04399005e-01  1.24813914e-02 -2.82148659e-01  1.39261514e-01\n",
      " -4.17274177e-01  3.38855529e+00  5.60585521e-02 -5.02046198e-04\n",
      "  1.19486213e-01 -4.68699075e-02  1.03447691e-01 -9.61723179e-02\n",
      " -1.86648130e-01 -1.35078952e-01  9.53821540e-02  9.29354280e-02\n",
      "  1.60715152e-02 -1.60932392e-02 -4.62598763e-02 -1.86912507e-01\n",
      " -1.30831692e-02  1.00406557e-01 -9.41457301e-02 -6.62912577e-02\n",
      " -1.91631317e-01  4.07526195e-01  2.97089834e-02  3.21937382e-01\n",
      "  1.36582345e-01 -1.28532505e+00  2.26989403e-01 -6.42775148e-02\n",
      "  3.18450332e-02  1.86309591e-01 -5.06213486e-01 -3.87956738e-01\n",
      " -8.22484791e-02 -2.30801940e-01  2.26057582e-02 -1.20151630e-02\n",
      " -4.42814827e-02  1.81877986e-01  1.26349404e-01  3.86309922e-01\n",
      " -7.98545480e-02  3.66022021e-01  2.22178996e-01  1.20482229e-01\n",
      " -1.08510979e-01 -6.59994930e-02 -9.40289721e-02 -2.77032703e-03\n",
      " -1.01610996e-01 -1.86663076e-01 -1.76684931e-03 -1.75535545e-01\n",
      "  7.81945139e-02  2.08004206e-01 -2.90989459e-01 -1.54725701e-01\n",
      " -6.31798208e-02 -5.57054803e-02 -1.27494603e-01  1.58422887e-01\n",
      " -3.58930439e-01 -2.49197438e-01  6.19344860e-02 -2.05597579e-01\n",
      "  6.94724321e-02  1.87529512e-02 -1.83054224e-01 -4.31398973e-02\n",
      " -4.10390407e-01 -3.02530980e+00 -2.20381200e-01 -1.97299197e-03\n",
      "  1.37724370e-01  2.11554170e-01  9.66469496e-02  1.08422726e-01\n",
      "  5.68172559e-02  8.60584974e-02 -4.79056537e-01  2.32665718e-01\n",
      " -1.48651466e-01 -5.64652570e-02  8.41620266e-02 -5.76793075e-01\n",
      "  3.72448921e-01  1.88859195e-01 -3.35847169e-01  1.13171734e-01\n",
      " -1.03446476e-01 -1.49084330e-02 -1.03951424e-01 -3.65483284e-01\n",
      "  4.04287912e-02  4.74641472e-03  1.57182902e-01 -1.45159647e-01\n",
      " -4.54698920e-01 -6.37674704e-03 -1.78421959e-02  7.97875077e-02\n",
      " -3.27782333e-02  2.74009109e-01 -5.43490797e-03 -5.12705818e-02\n",
      " -3.40946221e+00  1.96862280e-01  1.12679876e-01  5.10569476e-02\n",
      "  2.81257033e-01 -1.75176516e-01  3.96555483e-01 -2.54568040e-01\n",
      " -3.16779047e-01  1.05154894e-01  1.79801527e-02 -1.85128644e-01\n",
      "  7.15693235e-02 -3.27704940e-03  1.61979109e-01 -1.91637367e-01\n",
      "  6.24791384e-02 -3.47871423e-01  6.12868890e-02  1.31203458e-02\n",
      " -5.85916638e-02 -1.24528453e-01  1.03072934e-01  6.66169897e-02\n",
      "  5.15500009e-02  2.68464983e-01 -6.53296471e-01 -9.26016569e-02\n",
      " -2.61220075e-02 -9.68299061e-02 -1.52923405e-01 -3.80663395e-01\n",
      "  1.05169535e-01  9.81405601e-02 -2.28017569e-01 -1.98044658e-01\n",
      "  2.95836665e-02 -1.43358231e-01  2.34765202e-01  1.20771363e-01\n",
      " -4.17026609e-01  7.71915376e-01  2.92673349e-01  5.04469395e-01\n",
      "  6.60025716e-01  1.52636975e-01  1.15715206e-01 -2.16335967e-01\n",
      "  1.37534082e-01  5.11923768e-02 -1.10221334e-01  7.12175816e-02\n",
      "  8.86655390e-01  5.99123351e-02  3.77843417e-02 -5.53753003e-02\n",
      "  2.11428002e-01 -3.12885866e-02 -6.37753904e-02  1.54253840e-01\n",
      "  5.55341601e-01 -1.30185902e-01  4.46792580e-02  5.37758470e-01\n",
      "  1.85661197e-01 -4.21151847e-01  1.56861618e-01 -3.48945916e-01\n",
      " -9.28320065e-02  3.51028405e-02 -3.98329973e-01  2.75565267e-01\n",
      "  8.06543604e-02 -9.11333442e-01  4.65892404e-02 -7.65167251e-02\n",
      " -1.50869982e-02 -9.75890309e-02  1.18391149e-01  2.53755808e-01\n",
      " -3.87040585e-01  8.99522379e-02  9.46735591e-03  1.77825436e-01\n",
      " -2.43967816e-01 -2.26170942e-01 -9.41730589e-02  3.39686543e-01\n",
      " -5.58379032e-02 -1.33418441e-01 -1.92557245e-01  2.66441345e-01\n",
      "  2.04797864e-01  1.78879678e-01 -2.26428226e-01  2.16044411e-02\n",
      "  4.08956170e-01 -6.22725010e-01  7.35089481e-02  7.25795925e-02\n",
      "  3.51041555e-04 -2.48837262e-01 -2.99561143e-01  1.56859457e-01\n",
      " -4.04662043e-01 -4.98699158e-01 -1.51560828e-02  3.99244428e-01\n",
      "  1.04387194e-01  1.48904063e-02 -1.88013077e-01  1.90078199e-01\n",
      " -1.13267519e-01 -6.11442477e-02  1.01164603e+00 -8.28399435e-02\n",
      "  6.63716793e-02  3.90180558e-01 -2.49998912e-01  5.58492839e-02\n",
      "  1.26123041e-01  1.60635978e-01 -1.19132049e-01 -2.31224708e-02\n",
      " -3.65120709e-01 -8.15384649e-03 -2.41011009e-03 -2.11953551e-01\n",
      " -6.52023852e-02 -2.89857268e-01 -1.07636765e-01 -9.71890986e-02\n",
      " -3.69280338e-01 -5.97676098e-01 -8.46967846e-02 -6.62277415e-02\n",
      " -2.91235387e-01  7.61354417e-02  5.68732098e-02  1.49715975e-01\n",
      "  3.29563797e-01  3.25214744e-01 -8.32037032e-02  1.32066965e-01\n",
      " -5.31436726e-02  2.78652996e-01  1.62385792e-01  4.64542061e-01\n",
      "  3.79788727e-02  3.17493379e-01  1.81447148e-01  8.84629339e-02\n",
      " -8.08891356e-02 -3.98120314e-01  3.32667410e-01  1.98489830e-01\n",
      " -3.80491354e-02  6.98409602e-03 -6.84611034e-03  2.77163237e-01\n",
      "  1.61612913e-01  9.44201872e-02 -1.67623544e+00  9.52133536e-02\n",
      "  1.78901464e-01  1.82519257e-01  7.00872093e-02 -1.49754032e-01\n",
      " -9.24519449e-02  2.88561821e-01 -5.17775863e-02  2.14146942e-01\n",
      "  5.11426777e-02  2.62538884e-02  8.69940668e-02  8.61721188e-02\n",
      " -2.05659196e-02 -1.73070788e-01  1.91892907e-02  5.25253154e-02\n",
      "  1.01661496e-01  3.07802930e-02 -2.26105273e-01  1.19949162e-01\n",
      "  1.27450883e-01 -1.30369097e-01 -1.28789961e-01 -1.10537887e-01\n",
      " -1.93400383e-01  5.91374338e-02  3.35321687e-02  3.80296826e-01\n",
      "  1.08260199e-01 -4.02765393e-01 -8.01773608e-01 -1.96990132e-01\n",
      "  3.71227451e-02  1.48779094e-01  2.79367208e-01  9.42827687e-02\n",
      "  1.95166856e-01  1.80638373e-01 -2.31990978e-01  2.60695219e-01\n",
      "  3.88297379e-01 -1.36633962e-01  4.60659027e-01 -6.16911650e-02\n",
      " -1.20481245e-01  3.13234001e-01 -1.47788957e-01 -4.84861672e-01\n",
      " -2.50250325e-02 -9.12890397e-03 -8.03734660e-02 -1.56277403e-01\n",
      "  7.59765953e-02 -2.11633652e-01 -7.76903629e-02  2.61837780e-01\n",
      " -2.01588094e-01 -1.44949883e-01  2.37231553e-01 -3.73293906e-01\n",
      " -8.34490508e-02  9.98225063e-02 -1.23363197e-01 -6.28095448e-01\n",
      "  4.25763428e-03 -2.96611845e-01 -1.51879117e-01  2.02778518e-01\n",
      "  2.42679954e-01 -1.53355122e-01 -3.25158298e-01  1.85745999e-01\n",
      " -6.45620450e-02  2.55948097e-01 -6.70692474e-02  3.04700229e-02\n",
      " -1.80604845e-01 -1.24287859e-01 -5.53370640e-02  1.65590912e-01\n",
      "  1.67397469e-01  6.01246357e-01 -3.73725593e-03  1.76693127e-01\n",
      " -1.88665807e-01 -3.49957913e-01 -4.60853204e-02  5.60138375e-02\n",
      " -3.20114851e-01 -3.26906219e-02  4.30189967e-02 -7.99324960e-02\n",
      "  4.18973684e-01  2.74136662e-04  7.23115280e-02 -1.99644879e-01\n",
      " -2.28675604e-01 -1.72955155e-01 -5.88125736e-03  8.52202252e-02\n",
      "  1.07188657e-01  1.66747868e-01  3.97723690e-02 -8.21910203e-02\n",
      "  3.53084087e-01  9.12947208e-02 -2.79192507e-01 -1.58534780e-01\n",
      "  8.89005512e-03 -1.17894478e-01  1.13979965e-01  1.30665898e-02\n",
      "  1.85798794e-01 -8.46691132e-02 -4.19875145e-01 -2.34127522e-01\n",
      "  1.86110830e+00  6.27429426e-01  7.90965334e-02 -1.38845071e-02\n",
      "  2.41985366e-01 -1.51553690e-01  1.72620401e-01  3.34060967e-01\n",
      " -3.43732387e-02  3.10209721e-01 -3.07555348e-01 -2.02621147e-02\n",
      "  1.66980118e-01  2.95923471e-01  4.47136760e-01  1.75302505e-01\n",
      "  2.34065607e-01 -3.08059156e-01 -3.52235138e-01  5.29739708e-02\n",
      " -4.65174645e-01  8.18194672e-02  4.69030261e-01 -5.28732203e-02\n",
      "  4.18180078e-02  1.50904268e-01 -9.15888175e-02  3.08867544e-02\n",
      " -2.97810167e-01  2.70738125e-01  7.33202994e-02  2.14417666e-01\n",
      "  1.96445525e-01  1.27539694e-01  8.18801522e-02  1.01182558e-01\n",
      " -8.91114473e-02 -6.32148236e-02 -1.41968012e-01 -7.27884173e-02\n",
      "  6.61866739e-02 -1.67909592e-01  2.55457103e-01 -1.21357337e-01\n",
      "  2.24782705e-01  2.82310873e-01  1.75554752e-01 -2.71638006e-01\n",
      "  1.02203906e-01  3.31737876e-01 -1.84165761e-01  6.89989924e-02\n",
      " -1.84533060e-01  2.17834145e-01 -2.74252594e-01 -3.38261902e-01\n",
      "  7.56141916e-03 -1.00589633e-01  1.51510388e-01  1.14904895e-01\n",
      " -6.91391677e-02 -2.33401395e-02  2.32395023e-01  4.71375510e-03\n",
      "  2.63680257e-02 -5.09530902e-02 -4.19910103e-02  4.93295878e-01\n",
      " -1.62316442e-01  2.45528109e-03 -3.41578498e-02  3.03073138e-01\n",
      "  2.61684924e-01  1.06795944e-01  8.14694881e-01 -8.60971306e-03\n",
      "  2.48546809e-01  1.64035872e-01 -1.74412027e-01 -2.20285869e+00\n",
      " -2.10556258e-02  2.48907939e-01  2.77516723e-01 -7.01117441e-02\n",
      "  3.15820426e-01  6.15457237e-01 -1.31866261e-01 -5.20431772e-02\n",
      " -1.50904045e-01  1.05158016e-02  5.21955073e-01  2.89833158e-01\n",
      "  3.06981742e-01 -1.12640169e-02  5.54752350e-02  7.56801367e-02\n",
      "  1.19009502e-02 -1.70956068e-02  1.22089677e-01 -6.27313927e-03\n",
      " -2.75412686e-02  1.32253766e-02 -1.73716664e-01 -2.48824894e-01\n",
      "  2.54745930e-01  2.16185257e-01 -1.03973180e-01  5.70377335e-02\n",
      "  1.96898729e-03 -7.59445131e-02  2.06101224e-01 -1.31353825e-01\n",
      "  1.25363588e-01  3.39344233e-01 -1.69564486e-01 -1.01328269e-01\n",
      "  1.12412348e-01  9.18405652e-02 -1.12267751e-02 -1.30679920e-01\n",
      "  5.71299851e-01 -1.38593182e-01 -6.33092150e-02 -3.61461379e-02\n",
      " -1.48937777e-01  3.91469151e-01 -3.78400475e-01  3.35250974e-01\n",
      " -3.21930200e-01 -1.81722075e-01  1.09870687e-01 -1.73044503e-02\n",
      "  1.68115735e-01  2.17697054e-01  6.05825707e-03  1.17546566e-01\n",
      "  9.68239307e-02  1.89970434e-02 -5.16425073e-01  1.00806259e-01\n",
      " -3.92634049e-02 -1.37765817e-02 -1.43575724e-02  1.09012291e-01\n",
      " -2.28770062e-01 -1.00906372e-01  2.24642113e-01 -2.77896971e-01\n",
      " -1.59970820e-01 -1.16657382e-02 -3.08769703e-01 -1.83830410e-01\n",
      "  6.95849210e-02  1.43215328e-01  1.52753606e-01  4.72591370e-02\n",
      "  4.23039347e-01  2.81381190e-01 -1.24739660e-02 -6.72291219e-02\n",
      " -9.20454487e-02  7.53944367e-02  7.24847615e-02  8.86523575e-02\n",
      " -6.00563526e+00 -2.23558992e-01 -3.37932467e-01  1.06395833e-01\n",
      "  1.49221569e-01 -2.30137687e-02 -1.28527924e-01 -9.98526663e-02\n",
      "  6.29166290e-02 -2.80661434e-01  6.66360706e-02  1.36720866e-01\n",
      "  1.29787728e-01 -1.12066515e-01  4.16194826e-01  5.03380120e-01]\n",
      "-------------------------\n",
      "0.8203592814371258\n"
     ]
    }
   ],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PATH    = \"../datasets/\"\n",
    "FILE    = \"movie_reviewsBERT.csv\"\n",
    "batch_1 = pd.read_csv(PATH + FILE, delimiter=',', header=None)\n",
    "\n",
    "print(batch_1.shape)\n",
    "ROW =1\n",
    "print(\"Review 1st column: \" + batch_1.iloc[ROW][0])\n",
    "print(\"Rating 2nd column: \" + str(batch_1.iloc[ROW][1]))\n",
    "\n",
    "# Show counts for review scores.\n",
    "print(\"** Showing review counts\")\n",
    "print(batch_1[1].value_counts())\n",
    "\n",
    "# Load pretrained models.\n",
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel,\n",
    "                                                    ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model     = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "# Tokenize the sentences.\n",
    "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "print(\"\\n****************** Tokenized reviews \")\n",
    "print(tokenized)\n",
    "print(tokenized.values)\n",
    "print(\"******************\")\n",
    "\n",
    "# For processing we convert to 2D array.\n",
    "max_len = 0\n",
    "\n",
    "# Get maximum number of tokens (get biggest sentence).\n",
    "print(\"\\nGetting maximum number of tokens in a sentence\")\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "print(\"Most tokens in a review (max_len): \" + str(max_len))\n",
    "\n",
    "# Add padding\n",
    "print(\"------------\")\n",
    "print(\"Padded so review arrays as same size: \")\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "print(\"These are the padded reviews:\")\n",
    "print(padded)\n",
    "print(\"This is the last padded sentence:\")\n",
    "LAST_INDEX = len(batch_1) -1\n",
    "print(padded[LAST_INDEX])\n",
    "print(\"\\n------------\")\n",
    "print(\"Attention mask tells BERT to ignore the padding.\")\n",
    "\n",
    "# If we directly send padded data to BERT, that would slightly confuse it.\n",
    "# We need to create another variable to tell it to ignore (mask) the padding\n",
    "# we've added when it's processing its input. That's what attention_mask is\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "print(attention_mask.shape)\n",
    "print(attention_mask)\n",
    "print(attention_mask[LAST_INDEX])\n",
    "print(\"=============\")\n",
    "\n",
    "input_ids = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "print(\"Input ids which are padded reviews in torch tensor format:\")\n",
    "print(input_ids)\n",
    "print(\"Attention mask in torch tensor format:\")\n",
    "print(attention_mask)\n",
    "print(\"++++++++++++++\")\n",
    "\n",
    "# The model() function runs our sentences through BERT. The results of the\n",
    "# processing will be returned into last_hidden_states.\n",
    "print(\"BERT model transforms tokens and attention mask tensors into features \")\n",
    "print(\"for logistic regression.\")\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# We'll save those in the features variable, as they'll serve as the\n",
    "# features to our logitics regression model.\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "\n",
    "print(\"Let's see the features: \")\n",
    "print(features)\n",
    "print(features[1999])\n",
    "print(\"-------------------------\")\n",
    "\n",
    "from   sklearn.linear_model    import LogisticRegression\n",
    "from   sklearn.model_selection import train_test_split\n",
    "\n",
    "# The labels indicating which sentence is positive and negative\n",
    "# now go into the labels variable\n",
    "labels = batch_1[1]\n",
    "\n",
    "# Let's now split our datset into a training set and testing set (even\n",
    "# though we're using 2,000 sentences from the SST2 training set).\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n",
    "train_labels.describe()\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "print(lr_clf.score(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sentences Tokenized: \n",
      "[list([101, 2023, 8235, 3185, 2003, 5730, 1011, 7510, 1012, 102])\n",
      " list([101, 2023, 3185, 2003, 9643, 1012, 102])]\n",
      "----Padded------\n",
      "[[ 101 2023 8235 3185 2003 5730 1011 7510 1012  102    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 101 2023 3185 2003 9643 1012  102    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]]\n",
      "----------------\n",
      "------ Attention Mask ------\n",
      "[[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "------ Torch Attention Mask ------\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "------ Torch Padded ------\n",
      "tensor([[ 101, 2023, 8235, 3185, 2003, 5730, 1011, 7510, 1012,  102,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2023, 3185, 2003, 9643, 1012,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "BERT model transforms tokens and attention mask tensors into features \n",
      "for logistic regression.\n",
      "Let's see the features: \n",
      "[[ 0.11658795 -0.05706753  0.13648376 ... -0.20327923  0.47136134\n",
      "   0.16656114]\n",
      " [ 0.10359836 -0.01755353  0.10664839 ... -0.24315333  0.49144536\n",
      "   0.26117605]]\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 2001]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [22]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     51\u001B[0m labels \u001B[38;5;241m=\u001B[39m batch_1[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# Let's now split our datset into a training set and testing set (even\u001B[39;00m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# though we're using 2,000 sentences from the SST2 training set).\u001B[39;00m\n\u001B[0;32m---> 55\u001B[0m train_features, test_features, train_labels, test_labels \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m train_labels\u001B[38;5;241m.\u001B[39mdescribe()\n\u001B[1;32m     57\u001B[0m lr_clf \u001B[38;5;241m=\u001B[39m LogisticRegression(solver\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mliblinear\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/conda/miniforge3/envs/COMP-4949/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2417\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[1;32m   2414\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_arrays \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   2415\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one array required as input\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2417\u001B[0m arrays \u001B[38;5;241m=\u001B[39m \u001B[43mindexable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2419\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m   2420\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m _validate_shuffle_split(\n\u001B[1;32m   2421\u001B[0m     n_samples, test_size, train_size, default_test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m\n\u001B[1;32m   2422\u001B[0m )\n",
      "File \u001B[0;32m~/conda/miniforge3/envs/COMP-4949/lib/python3.9/site-packages/sklearn/utils/validation.py:378\u001B[0m, in \u001B[0;36mindexable\u001B[0;34m(*iterables)\u001B[0m\n\u001B[1;32m    359\u001B[0m \u001B[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \n\u001B[1;32m    361\u001B[0m \u001B[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001B[39;00m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    377\u001B[0m result \u001B[38;5;241m=\u001B[39m [_make_indexable(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m iterables]\n\u001B[0;32m--> 378\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/conda/miniforge3/envs/COMP-4949/lib/python3.9/site-packages/sklearn/utils/validation.py:332\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[0;34m(*arrays)\u001B[0m\n\u001B[1;32m    330\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[1;32m    331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 332\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    333\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    334\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[1;32m    335\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [2, 2001]"
     ]
    }
   ],
   "source": [
    "dfEx = pd.DataFrame(columns=[0,1])\n",
    "dfEx = dfEx.append({0:\"This brilliant movie is jaw-dropping.\", 1:1},\n",
    "                   ignore_index=True)\n",
    "dfEx = dfEx.append({0:\"This movie is awful.\", 1:0}, ignore_index=True)\n",
    "\n",
    "tokenized_lines = dfEx[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "print(\"\\n Sentences Tokenized: \")\n",
    "print(tokenized_lines.values)\n",
    "\n",
    "# padding\n",
    "padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized_lines.values])\n",
    "print(\"----Padded------\")\n",
    "print(padded)\n",
    "print(\"----------------\")\n",
    "\n",
    "# add attention mask\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "print(\"------ Attention Mask ------\")\n",
    "print(attention_mask)\n",
    "\n",
    "# Convert attention mask in tensor format\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "print(\"------ Torch Attention Mask ------\")\n",
    "print(attention_mask)\n",
    "\n",
    "# Convert padded reviews in tensor format\n",
    "input_ids = torch.tensor(padded)\n",
    "print(\"------ Torch Padded ------\")\n",
    "print(input_ids)\n",
    "\n",
    "# The model() function runs our sentences through BERT. The results of the\n",
    "# processing will be returned into last_hidden_states.\n",
    "print(\"BERT model transforms tokens and attention mask tensors into features \")\n",
    "print(\"for logistic regression.\")\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# We'll save those in the features variable, as they'll serve as the\n",
    "# features to our logitics regression model.\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "\n",
    "print(\"Let's see the features: \")\n",
    "print(features)\n",
    "print(\"-------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff5f5e26eb4c41888260a01f9b3921be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f7990695de9463eb9d6559c547152ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13267fb11adb474782ea2f4670438af8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e33196189b54098b9738d10c75c69ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef4229327cf449868b54e5d3b3971c34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126    Just so that you know,yetunde hasn't sent mone...\n",
      "75           I am waiting machan. Call me once you free.\n",
      "90     Yeah do! Don‰Û÷t stand to close tho- you‰Û÷ll ...\n",
      "48     Yeah hopefully, if tyler can't do it I could m...\n",
      "119    PRIVATE! Your 2004 Account Statement for 07742...\n",
      "169    Yes :)it completely in out of form:)clark also...\n",
      "144                            Yes see ya not on the dot\n",
      "38                           Anything lor... U decide...\n",
      "114    Wa, ur openin sentence very formal... Anyway, ...\n",
      "57                    Sorry, I'll call later in meeting.\n",
      "98     Hi. Wk been ok - on hols now! Yes on for a bit...\n",
      "10     I'm gonna be home soon and i don't want to tal...\n",
      "162    I'm so in love with you. I'm excited each day ...\n",
      "65     As a valued customer, I am pleased to advise y...\n",
      "33     For fear of fainting with the of all that hous...\n",
      "124             ÌÏ predict wat time Ì_'ll finish buying?\n",
      "95     Your free ringtone is waiting to be collected....\n",
      "89             Ela kano.,il download, come wen ur free..\n",
      "84                                        Yup next stop.\n",
      "159    You are a winner U have been specially selecte...\n",
      "52     K fyi x has a ride early tomorrow morning but ...\n",
      "40     Pls go ahead with watts. I just wanted to be s...\n",
      "142    A swt thought: \\Nver get tired of doing little...\n",
      "141                          Sir, Waiting for your mail.\n",
      "66     Today is \\song dedicated day..\\\" Which song wi...\n",
      "86     For real when u getting on yo? I only need 2 m...\n",
      "Name: text, dtype: object\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Class Weights: [0.6122449  2.72727273]\n",
      "Class Weights: [0.6122449  2.72727273]\n",
      "\n",
      " Epoch 1 / 10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [25]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m Epoch \u001B[39m\u001B[38;5;132;01m{:}\u001B[39;00m\u001B[38;5;124m / \u001B[39m\u001B[38;5;132;01m{:}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, epochs))\n\u001B[1;32m    273\u001B[0m \u001B[38;5;66;03m# train model\u001B[39;00m\n\u001B[0;32m--> 274\u001B[0m train_loss, _ \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    275\u001B[0m valid_loss, _ \u001B[38;5;241m=\u001B[39m evaluate()\n\u001B[1;32m    277\u001B[0m \u001B[38;5;66;03m# save the best model\u001B[39;00m\n",
      "Input \u001B[0;32mIn [25]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    181\u001B[0m model\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    183\u001B[0m \u001B[38;5;66;03m# get model predictions for the current batch\u001B[39;00m\n\u001B[0;32m--> 184\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msent_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;66;03m# compute the loss between actual and predicted values\u001B[39;00m\n\u001B[1;32m    187\u001B[0m loss \u001B[38;5;241m=\u001B[39m cross_entropy(preds, labels)\n",
      "File \u001B[0;32m~/conda/miniforge3/envs/COMP-4949/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[0;32mIn [25]\u001B[0m, in \u001B[0;36mBERT_Arch.forward\u001B[0;34m(self, sent_id, mask)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, sent_id, mask):\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;66;03m# pass the inputs to the model\u001B[39;00m\n\u001B[1;32m    108\u001B[0m     _, cls_hs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert(sent_id, attention_mask\u001B[38;5;241m=\u001B[39mmask)\n\u001B[0;32m--> 109\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcls_hs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    110\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m    111\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n",
      "File \u001B[0;32m~/conda/miniforge3/envs/COMP-4949/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/conda/miniforge3/envs/COMP-4949/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/conda/miniforge3/envs/COMP-4949/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001B[0m, in \u001B[0;36mlinear\u001B[0;34m(input, weight, bias)\u001B[0m\n\u001B[1;32m   1846\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight, bias):\n\u001B[1;32m   1847\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(linear, (\u001B[38;5;28minput\u001B[39m, weight, bias), \u001B[38;5;28minput\u001B[39m, weight, bias\u001B[38;5;241m=\u001B[39mbias)\n\u001B[0;32m-> 1848\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: linear(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from joblib.logger import format_time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "PATH = \"../datasets/\"\n",
    "df   = pd.read_csv(PATH + \"spamdata_lite.csv\")\n",
    "df.head()\n",
    "\n",
    "# Create training set.\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], random_state=2018,\n",
    "                                                                    test_size=0.3, stratify=df['label'])\n",
    "\n",
    "# Use temp set from above to create validation and test set.\n",
    "# Validation is used to test the model while training. Test is used\n",
    "# to validate the model after training.\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, random_state=2018,\n",
    "                                                                test_size=0.5, stratify=temp_labels)\n",
    "\n",
    "# import BERT-base pretrained model.\n",
    "# We are using weights that are suitable for uncased content. However if\n",
    "# upper and lower case words are relevant for your domain use cased.\n",
    "bertModel = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode sentences.\n",
    "def prepXandY(text, labels):\n",
    "    textList = text.tolist()\n",
    "    tokens = tokenizer.batch_encode_plus(\n",
    "        textList,\n",
    "        max_length=25,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    seq = torch.tensor(tokens['input_ids'])\n",
    "    mask = torch.tensor(tokens['attention_mask'])\n",
    "    y = torch.tensor(labels.tolist())\n",
    "\n",
    "    return seq, mask, y\n",
    "\n",
    "print(test_text)\n",
    "\n",
    "# Prepare the data.\n",
    "def getTensor(text, labels):\n",
    "    seq, mask, y = prepXandY(train_text, train_labels)\n",
    "    tensorData = TensorDataset(seq, mask, y)\n",
    "    return tensorData\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = getTensor(train_text, train_labels)\n",
    "val_data   = getTensor(val_text, val_labels)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# freeze all the parameters\n",
    "for param in bertModel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "        # softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        # pass the inputs to the model\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bertModel)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5)  # learning rate\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# compute the class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# converting list of class weights to a tensor\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# push to GPU (if it exists)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy = nn.NLLLoss(weight=weights)\n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# compute the class weights\n",
    "class_weights = compute_class_weight(class_weight ='balanced',\n",
    "                                     classes=np.unique(train_labels),\n",
    "                                     y=train_labels)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# function to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    # returns the loss and predictions\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# for each epoch\n",
    "for epoch in range(epochs):\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "\n",
    "    # train model\n",
    "    train_loss, _ = train()\n",
    "    valid_loss, _ = evaluate()\n",
    "\n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "# load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Validate with test data.\n",
    "test_seq, test_mask, test_y = prepXandY(test_text, test_labels)\n",
    "\n",
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "# %%\n",
    "print(\"Show preds before report\")\n",
    "print(preds)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print(\"Preds after argmax\")\n",
    "print(preds)\n",
    "print(classification_report(test_y, preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}